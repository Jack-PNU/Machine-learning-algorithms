    Adaboost算法是由Boosting算法改进而来，Boosting算法是一种提高任意给定学习算法准确度的方法（详情自行百度），Valiant和Kearns提出了弱学习和强学习的概念，
 识别错误率小于1/2,也即准确率仅比随机猜测略高的学习算法称为弱学习算法;识别准确率很高并能在多项式时间内完成的学习算法称为强学习算法。 1995年, Freund和
 schapire改进了Boosting算法,提出了AdaBoost(Adaptive Boosting)算法,该算法效率和Freund于1991年提出的Boosting算法几乎相同,但不需要任何关于弱学习器的先验
 知识,因而更容易应用到实际问题当中。之后,Freund和schapire进一步提出了改变Boosting投票权重的AdaBoost.M1,AdaBoost.M2等算法,在机器学习领域受到了极大的关注。

一、实际应用：
     Adaboost是一个分类算法，主要解决了: 两类问题、多类单标签问题、多类多标签问题等问题。现在主要应用在人脸检测方面，效果很好，另还有目标识别、计算机视觉领域
 等等。
 
 二、算法原理：
 1、Adaboost是什么
    Adaboost是英文"Adaptive Boosting"（自适应增强）的缩写，它的自适应在于：前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类
 类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。（就是俗话说的三个臭皮匠顶个诸葛亮）举个例子，
     
